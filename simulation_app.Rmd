---
title: "CI_simulation"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,echo=FALSE, message=FALSE}
require(shiny)
require(shinyjs)
require(tidyverse)
theme_set(theme_bw())
```

# 1. Introduction

Welcome to the simulation app! This app is intended for students who have some basic statistics knowledge and little to no knowledge of causal inference (R knowledge is optional). Throughout your simulation journey, we will cover several different topics using a hypothetical real-world example that will hopefully build your intuition and knowledge about the joys of simulation. One key thing to remember is that as you go through this app, you will have access to two very important hats: the _researcher_ hat, and the _omniscient_ hat. The _researcher_ hat is one you wear daily - you are in the real world, and have normal human limitations. However, every once in a while, you will get to wear the _omniscient_ hat, where you will transcend your feeble human mind and become an all-knowing and powerful being. These two hats are very important when we are simulating, and will come into play quite often throughout our journey. Ready? Okay, let's get started.

### Hypothetical real-world example
In New York State, there are a series of exams that high school students must take and pass in order to graduate. These are known as the Regents exams, and they cover a wide variety of topics such as Algebra, Global History, English, and Earth Science. Generally, the Global History regents is considered to be one of the most difficult, primarily due to the sheer amount of memorization students need to do (it is a traumatizing memory for this author). Students must score a 65 or higher to pass, but may re-take these exams as many times as necessary until they achieve their desired score. However, these exams are generally offered once a year, so students wishing to re-take the exam must wait a year. 

Let's say you are a budding educational researcher who wants to understand how an afterschool pilot program would impact students' Global History regents scores. However, you are but one researcher, and don't have the funding or a team to help collect the necessary data. You also obviously don't have an actual afterschool pilot program either. But this is a question that has burned with the intensity of a thousand suns ever since you were in high school! What if you had access to an afterschool program back then? Would you have done better on your Global History regents exam?


### Where do we start?
Since this is a hypothetical example, we physically don't have any data. And as such, you can do one of two things to answer your research question: travel back in time, or simulate. 
If you decide to go with option one, let me know - I have some scores I want settled with some people in my past. 
More than likely however, you will need to _simulate your data_ to answer your question. Luckily, you have those two hats, which will open up worlds of possibilities, and be the key to solving this burning question of yours.


### What is simulation?
Simulation in broad terms, is a research or teaching technique that reproduces actual events and processes under test conditions. It is often used in industry, science, and education, and developing one is often a highly complex mathematical process. Initially a set of rules, relationships, and operating procedures are specified, along with other variables. The interaction of these phenomena create new situations, even new rules, which further evolve as the simulation proceeds. Simulation implementations range from paper-and-pencil and board-game reproductions of situations to complex computer-aided interactive systems.

A simulation imitates the operation of real world processes or systems with the use of models. The model represents the key behaviours and characteristics of the selected process or system, while the simulation represents how the model evolves under different conditions over time. Simulating your data also means you have the power to determine what the data are and how they get generated. All of this is called the _Data Generating Process_, and will be discussed in detail later in this app. 

Let's contextualize the broader definition of simulation to our hypothetical example, and put the _omniscient_ hat on, since it feels good to know everything.
With this powerful hat, you know that you have a sample of 100 fake students (that you will generate), who must be randomly assigned (by some model you will specify) into treatment and control groups. "Treatment" in this case is the afterschool program, and thus those in the treatment group will go through the afterschool program, and those in the control group will receive normal tutoring. You will also be generating all of these students' "pre-treatment" test scores (again, through models), as well as their "post-treatment" test scores, otherwise known as the _outcomes_. These are fairly self-explanatory: **pre-treatment** test scores are the scores of the 100 sample students prior to any of them going through the afterschool program, and **post-treatment** test scores are the scores of the 100 sample students after the treatment period. The great thing about being omniscient is that you will also be able to see what the treatment group students' scores are if they don't receive the treatment - these are called _potential outcomes_. 

For comparison, let's switch to the _researcher_ hat for a moment to see the difference. As a mere researcher, you would still see the post-treatment scores for everyone, but you cannot know what the post-treatment test scores of the treatment group students would be _if they hadn't received the treatment_ (unless you can time travel, which again, let me know). The beauty of simulation is that it allows you to overcome this meta-physical roadblock to create a sort of "parallel universe" where, everything else being exactly the same, students in the treatment group never received the treatment. This is key to making causal inference.

For now, understand that simulation starts all the way at the beginning: who is in your sample, and what are their pre-treatment test scores? 

This is an example of how a model (that you specify) would randomly select 50 students to be put into the treatment group. When you are randomly assigning students to receive the treatment, your model picks 50 students randomly. Thus, everytime your model runs, each simulation will result in a different group of 50 students. Try it for yourself!

> Example: The possible combinations of students who would receive the treatment
> All 100 students: James, Robert, John, Michael, William, David, Richard, Joseph, Thomas, Charles, Christopher, Daniel, Matthew, Anthony, Mark, Donald, Steven, Paul, Andrew, Joshua, Kenneth, Kevin, Brian, George, Edward, Mary, Patricia, Jennifer, Linda, Elizabeth, Barbara, Yeri, Jessica, Sarah, Karen, Nancy, Lisa, Lee, Margaret, Sandra, Ashley, Kimberly, Emily, Donna, Michelle, Dorothy, Carol, Amanda, Melissa, Hee-jung


```{r, echo=FALSE}
students <- c('James','Robert', 'John',
'Michael', 'William', 'David', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 
'Matthew', 'Anthony', 'Mark', 'Donald', 'Steven', 'Paul', 'Andrew', 'Joshua', 'Kenneth', 'Kevin', 
'Brian', 'George', 'Edward', 'Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara',
'Yeri', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Lisa', 'Lee', 'Margaret', 'Sandra', 'Ashley',
'Kimberly', 'Emily', 'Donna', 'Michelle', 'Dorothy', 'Carol', 'Amanda', 'Melissa', 'Hee-jung')
fluidPage(
  actionButton("draw_50_student", "Draw 50 students for treatment"),
  textOutput('student_list')
)
###not enough names yet, so 25 for now
observeEvent(input$draw_50_student, {
  studentlist <- sample(students, size = 25)
  output$student_list <- renderText(paste0(studentlist))
})
```

Since the only limitation of simulation is computational, it can generate as many new results as you need, and represents the characterstics of the model(s) you defined. Now that you have a taste of what simulation looks like in its simplest form, let's take a short step back to really understand the underlying models and their distributional properties, since these really make up the core of your simulation study. 


# 2. What is probability?

## 2.1 Random Variable and Probability Distributions

![](./img/population.png)

Suppose you're drawing students with pre-treatment regents test scores between 0 and 100 from high schools in New York state. When a student (let's call them Ming) is drawn at random, the corresponding “random variable” is Ming's pre-treatment score. If we repeat this process for Ming, the probability of getting each possible score from the random variable is its distribution. Distributions are about the population. In this example, the probability distribution of the random variable (pre-treatment test scores) specifies the probabilities of all possible outcomes, such as the probability of a test score equaling 15, 65, 100, and so on.

You can think about any regular (deterministic) function like a box. For example, for the function $f(x) = x^2$, *every time* you give the function the value 2, it will *always* give you back 4. However, random variables are random functions that map the sample space (different students) into the real line (test score values between 0 and 100). This means that each time you apply the function it will give you a different number.

**Vera NOTE: Just edit the example to be a bit more gender-neutral.**


## Discrete and continuous random variable

There are two main types of random variables: _discrete_ and _continuous_.

- *Discrete random variables* can only take on a countable number of values (possibly infinite, but oftentimes finite). These are things like the number of Heads in two fair coin tosses, or the number of students assigned to a treatment condition (hint for later).

> Treatments randomly assigned to a student (i.e., a binary variable of 0 or 1) or the proportion of 1 in the treatments randomly assigned to 100 students 

```{r, echo=FALSE}
fluidPage(
  actionButton("one_student_treatment", "Assign a student"),
  textOutput('one_student_treatment_plot')
)
observeEvent(input$one_student_treatment, {
  treatment <- rbinom(1,size = 1, prob = 0.5)
  student <- sample(students, size = 1)
  output$one_student_treatment_plot <- renderText(paste0(student, ': ', treatment))
})
```

```{r, echo=FALSE}
fluidPage(
  actionButton("hundred_student_treatment", "Assign 100 students"),
  plotOutput('hundred_students_treatment_plot')
)

df <- reactiveValues(treatment = c(0,0,0))

observeEvent(input$hundred_student_treatment, {
  df$treatment <- rbinom(100, size = 1, prob = 0.5)
  output$hundred_students_treatment_plot <- renderPlot({
    tmp <- data.frame(treatment = df$treatment)
    ggplot() + geom_histogram(data = tmp, aes(x = treatment, y = ..density..), bins = 30, alpha = 0.5) 
    })
})
```


- *Continuous random variables* can take on any real number, an uncountable amount of possibilities (i.e., to any amount of decimal places).

> Example: pre-treament score of a student randomly drawn, or the mean of pre-treament scores of 100 students randomly drawn


```{r, echo=FALSE}
students <- c('James','Robert', 'John',
'Michael', 'William', 'David', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 
'Matthew', 'Anthony', 'Mark', 'Donald', 'Steven', 'Paul', 'Andrew', 'Joshua', 'Kenneth', 'Kevin', 
'Brian', 'George', 'Edward', 'Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara',
'Susan', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Lisa', 'Betty', 'Margaret', 'Sandra', 'Ashley',
'Kimberly', 'Emily', 'Donna', 'Michelle', 'Dorothy', 'Carol', 'Amanda', 'Melissa', 'Deborah')
fluidPage(
  actionButton("draw_a_student", "Draw a student"),
  textOutput('one_student_score')
)
observeEvent(input$draw_a_student, {
  score <- round(rnorm(1,60,15),2)
  student <- sample(students, size = 1)
  output$one_student_score <- renderText(paste0(student, ': ', score))
})
```

```{r, echo=FALSE}
fluidPage(
actionButton("draw_hundred_student", "Draw 100 students"),
plotOutput('hundred_students_scores')
)

df <- reactiveValues(score = c(0,0,0))

observeEvent(input$draw_hundred_student, {
  df$score <- rnorm(100,60,10)
  output$hundred_students_scores <- renderPlot({
    tmp <- data.frame(score = df$score)
    ggplot() + geom_histogram(data = tmp, aes(x = score, y = ..density..), bins = 30, alpha = 0.5) + 
      geom_vline(xintercept = mean(tmp$score), color = 'blue') 
    })
})
```


## Mean and standard deviation of a probability distribution

A probability distribution of a random variable $Z$ takes on some range of values (pre-treatment regents test scores between 0 and 100 of students from high schools in New York state). The mean of this distribution is the average of all these scores or, equivalently, the score that would be obtained on average from a random sample from the distribution. The mean is also called the expectation or expected value and is written as $E(Z)$ or $\mu_Z$ . For example, the plot below shows the (approximate) distribution of the pre-treatment regents test scores of students from high schools in New York state. The mean of this distribution is 60: this is the average score of all the high school students in New York and it is also the average score we would expect to see from sampling one student at random.

The variance of the distribution of $Z$ is $E((Z − \mu_Z )^2)$, that is, the mean of the squared difference from the mean. To understand this expression, first consider the special case in which $Z$ takes on only a single score In that case, this single value is the mean, so $Z − \mu_Z = 0$ for all $Z$ in the distribution, and the variance is 0. To the extent that the distribution has variation, so that sampled scores from high school in New York State can be different, this will show up as values of $Z$ that are higher and lower than $\mu_Z$, and the variance of $Z$ is nonzero.

The standard deviation is the square root of the variance. We typically work with the standard deviation rather than the variance because it is on the original scale of the distribution. In the plot below, the standard deviation of students' scores is 10: that is, if you randomly sample a student from the population, observe their score z, and compute (z − 60)^2, then the average value you will get is 100; this is the variance, and the standard deviation is $\sqrt(100) = 10$. 

```{r, echo=FALSE}
fluidPage(
actionButton("draw_sample", "Draw students"),
plotOutput('mean_var')
)


df <- reactiveValues(data = c())

observeEvent(input$draw_sample, {
  df$data <- rnorm(10000,60,10)
  output$mean_var <- renderPlot({
    tmp <- data.frame(data = df$data)
    mean <- paste0('Mean: ', round(mean(tmp$data),1))
    sd <- paste0('Standard Deviation: ', round(sd(tmp$data),1))
    ggplot() + geom_histogram(data = tmp, aes(x = data, y = ..density..), 
                 bins = 30, alpha = 0.5) + 
      geom_vline(xintercept = mean(tmp$data)) +
      annotate("text",x=90,y=0.035,label=as.character(mean), fontface = "italic", size = 6) +
      annotate("text",x=90,y=0.032,label=as.character(sd), fontface = "italic", size = 6) 
    })
})
```



# Page 2

## Normal Distribution

Probably one of the most famous distributions in all of statistics is the _Normal distribution_. This is a distribution of a  continuous random variable, and is often used in simulation and teaching because it has very nice properties. 
**HC NOTE: Expand a little more to bring to simulating students' test scores**

In addition to the distribution of data, probabilistic distributions are also commonly used in regression modeling to help us characterize the variation that remains after predicting the average --- the error term $\epsilon$ in the expression $y = a + bx + \epsilon$. These distributions allow us to get a handle on how uncertain our predictions are and, additionally, our uncertainty in the estimated parameters of the model.


```{r, echo=FALSE}
# shinyApp(
#   ui = shinyUI(
fluidPage(
  tags$div(
    sliderInput('normal_size', label = "Sampel Size", min = 10, max = 5000, value = 50, step = 1),
    numericInput(inputId = "select_normal_mean", "Mean:", 0),
    numericInput(inputId = "select_normal_sd", "Standard Deviation:", 1, max = 100)
  ),
  plotOutput('normal'),
  tags$div(
    useShinyjs(),
    actionButton("show_code_normal", "Show me the R code of generating the distribution"),
    hidden(
      div(id='code_div_normal',
          verbatimTextOutput("code_normal")
      )
    )
  ))
# ),
# server = function(input, output, session){

output$normal <- renderPlot({
  tmp <- data.frame(data = rnorm(n = input$normal_size, mean = input$select_normal_mean, sd = input$select_normal_sd))
  ggplot() + geom_histogram(data = tmp, aes(x = data, y = ..density..), bins = 30, alpha = 0.5)
})
observeEvent(input$show_code_normal, {
  toggle('code_div_normal')
  output$code_normal <- renderText({
    paste0('rnorm(n = ', input$normal_size, ', mean = ', input$select_normal_mean, ', sd = ', input$select_normal_sd, ')')
  })
  
}
)
# }
# )
```


## Binomial Distribution

We've discussed discrete random variables as taking on a countable number of values. A famous distribution of discrete random variables that is often used as models for data is called the _Binomial distribution_. This distribution has two parameters, $n$ and $p$, where $n$ is the number is "trials", and $p$ is the probability of "success". 

For example, let's say we flip a fair coin 100 times ("trials"), and count the number of Heads ("success"). The random variable $X$ is the number of Heads we observe, and the distribution of this random variable (Binomial) is shown here as an example.

```{r, echo=FALSE}
fluidPage(
  tags$div(
    sliderInput('binom_size', label = "Sample Size", min = 10, max = 5000, value = 50, step = 1),
    numericInput(inputId = "select_binom_size", "number of trials:", value = 10, min = 1),
    numericInput(inputId = "select_binom_prob", "probability of success on each trial:", value = 0.5, min = 0, max = 1, step = 0.1)),
    plotOutput('binomial'),
    tags$div(
      useShinyjs(),
      actionButton("show_code_binomial", "Show me the R code of generating the distribution"),
      hidden(
        div(id='code_div_binomial',
            verbatimTextOutput("code_binomial")
        )
      )
    ))

output$binomial <- renderPlot({
  tmp <- data.frame(data = rbinom(n = input$binom_size, size = input$select_binom_size, prob = input$select_binom_prob))
  ggplot() + geom_histogram(data = tmp, aes(x = data, y = ..density..), bins = 30, alpha = 0.5)
  })
observeEvent(input$show_code_binomial, {
  toggle('code_div_binomial')
  output$code_binomial <- renderText({
    paste0('rbinom(n = ', input$binom_size, ', size = ', input$select_binom_size, ', porb = ', input$select_binom_prob, ')')
  })
  
}
)
```

Now let's take this idea and apply it to our example. After we have generated the sample of 100 students and their respective test scores, we need a way to randomly assign 50 of them to the treatment group and 50 of them to the control group. The binomial distribution provides an excellent model to generate this "treatment assignment". 

```{r random_assign, echo=FALSE}
# SIMULATION EXAMPLE
  
```

## Poisson Distribution - **KEEP FOR NOW BUT CONSIDER REMOVING FOR ACTUAL LAUNCH - HC**

$p(x) = \lambda^x e^{\frac{-\lambda}{x!}}$


```{r, echo=FALSE}
inputPanel(
  sliderInput('pois_size', label = "Sampel Size", min = 10, max = 5000, value = 50, step = 1),
  numericInput(inputId = "select_pois_lambda", "Mean", value = 5, min = 0)
)

renderPlot({
  tmp <- rpois(n = input$pois_size, lambda = input$select_pois_lambda)
  hist(tmp)
  })
```


# Page 3
## Sampling Distribution
The sampling distribution is the set of possible datasets that could have been observed if the data collection process had been re-done, along with the probabilities of these possible values. 

The simplest example of a sampling distribution is the pure random sampling model: if the data are a simple random sample of size n from a population of size N, then the sampling distribution is the set of all samples of size n, all with equal probabilities.

The normal distribution, binomial distribution, and poisson distribution with specified parameters on the previous page are all sampling distributions for the samples of sizes of your choice.

> Say there are 10,000 NYU grad students and we randomly select 1000 students. Here we have population size of 10,000 and sample size of 1000. How many samples of size “n” are possible out of a population of size “N”? That's 10000 choose 1000, ${1000 \choose100}$, and the number is so large that even R only returns Inf.

```{r}
choose(10000,1000)
```


The next simplest example is pure measurement error: if observations $y_i$, i = 1,. . . , n, are generated from the model $y_i = a + bx_i + \epsilon_i$ , with fixed coefficients a and b, pre-specified values of the predictor $x_i$ , and a specified distribution for the errors $\epsilon_i$ (for example, normal with mean 0 and standard deviation $\sigma$), then the sampling distribution is the set of possible datasets obtained from these values of $x_i$ , drawing new errors $\epsilon_i$ from their assigned distribution.

```{r, echo=FALSE}
fluidRow(
  p("One continuous predictor: y = b0 + b1x + eps"),
  numericInput(inputId = "select_b0", "Intercept (b0):", 1),
  numericInput(inputId = "select_b1", "Coefficient on X (b1):", 0.5),
  numericInput(inputId = "select_sigma", "Residual Std Dev (sigma):", 1),
   sliderInput(inputId = "sample_size",label = "Select Sample Size",
    min = 10, max = 1000, value = 250, step = 10),
  plotOutput('regression'))

output$regression <- renderPlot({
  x <- seq(from = 1, to = 10, length.out = input$sample_size)
  y <- input$select_b0 + input$select_b1*x + rnorm(length(x), 0,input$select_sigma)
  df <- data.frame(x, y)
  ggplot(df, aes(x = x, y = y)) + geom_point() + geom_smooth(method='lm', formula= y~x,se = F) + theme_bw()
})

```



In practice, we will not know the sampling distribution; we can only estimate it, as it depends on aspects of the population, not merely on the observed data. In the pure random sampling model, the sampling distribution depends on all N datapoints. For the measurement-error model, the sampling distribution depends on the parameters a, b, and $\sigma$, which in general are not known, and will be estimated from the data. 



## Dependence
### Recognizing Conditional Probabilities
- “A blood test indicates the presence of a disease 95% of the time the disease is actually present”

> Translation: “P(test indicates disease | has disease)=0.95”

- “Males who have a circulation problem are twice as likely to be smokers as those who do not have a circulation problem.”

> Translation: “P(smoker | circulation problem)=2 P(smoker | no circulation problem)”

If the the probability of an event A is different from the conditional probability of A given a second event B, That is, $P(A|B) \neq P(A)$. Then we say that A and B are dependent. Another way to think about this is: knowing that B has occurred gives us different information about the probability of A also occurring than we have not knowing anything about B.
- Examples of dependent events:

> A: Wearing sunscreen and B: Eating ice cream 

> A: Midterm score and B: Pass the course or not

* Note that dependent events are not necessarily causally related.

```{r, echo=FALSE}
shinyApp(
  ui = shinyUI(fluidPage(
    useShinyjs(), 
    actionButton("show_code_sunscreen", "Show me the R code for the sunscreen example"),
    hidden(
      div(id='code_div_sunscreen',
        verbatimTextOutput("code_sunscreen")
        )
    ),
    actionButton("show_code_midterm", "Show me the R code for the midterm example"),
    hidden(
      div(id='code_div_midterm',
        verbatimTextOutput("code_midterm")
        )
    )
  )
  ),
  server = function(input, output, session){
    observeEvent(input$show_code_sunscreen, {
      toggle('code_div_sunscreen')
      output$code_sunscreen <- renderText( 
        paste0('sunscreen <- rbinom(10, size = 1, prob = 0.6)', '\n',
'ice_cream <- rbinom(10, size = 1, prob = (sunscreen + 0.1)*0.8)'))
    })
    observeEvent(input$show_code_midterm, {
      toggle('code_div_midterm')
      output$code_midterm <- renderText(
        paste0('midterm <- runif(10, min = 10, max = 95)','\n',
'p <- exp(midterm*0.01)/(1+exp(midterm*0.01))', '\n',
'final <- rbinom(10, size = 1, prob = p)')
      )
    })

  }
)
```

What happens if knowing B gives us no new information about A? Independent events are events that do not affect each other

> A: It is Monday and B: It is raining outside

> A: I eat oatmeal for breakfast and B: I am wearing jeans.

## Simulation

Simulation of random variables is important in applied statistics for several reasons. 
First, we use several probability models to mimic variation in the world, and the tools of simulation can help us better understand how this variation plays out. 

Second, we can use simulation to approximate the sampling distribution of data and propagate this to the sampling distribution of statistical estimates and procedures. 

Third, regression models are not deterministic; they produce probabilistic predictions. Simulation is the most convenient and general way to represent uncertainties in forecasts. 

In this final section, we will use what we learned and simulated in the previous sections to answer the question you were initially tasked with at the beginning: Is the afterschool program effective in improving high school students' scores on the Global History regents exam? Remember that _omniscient_ hat? It's time to put it on.

As with any simulation study, we need to first establish our **Data Generating Process (DGP)**. This means explicitly stating how you will be generating all of the data you need to estimate the treatment effect later on. For the purposes of this study, we will use what we learned in previous sections to walk through our DGP. 

### DGP

#### 1. Treatment Assignment
We already know how to generate treatment assignments from [section 2] using the Binomial distribution. The probability of assignment will be _0.5_.:
[RSHINY/CODE]

#### 2. Generating pre-treatment test scores
We also know that we can use the Normal distribution from [section 2] to generate our pre-treatment test scores. Remember: these are the original test scores of all the students prior to any of them attending the afterschool program.
[RSHINY/CODE]

#### 3. Generating outcome test scores based on treatment assignment
Here we start to make good use of our omniscient hat. As omniscient beings, we know that the treatment effect (or \tau) is **5**. That is, we know that the post-treatment test scores of students who went through the afterschool program is on average **5** points higher than the students who did not. To generate these outcome scores, we would simulate a _dependency_ based on the treatment assignment variable from above:
[RSHINY/CODE showing Y0 and Y1 based on treatment assignment]

### Calculating the Average Treatment Effect (ATE)
Note that we use "calculating" instead of "estimating". This is intentional, and is meant to illustrate the difference in process when you are wearing the omniscient hat versus the researcher hat. Specifically, as a researcher, you are always _estimating_ the ATE (or any other estimand) because we will never know the truth (in this case, that the treatment effect is 5). But when you are simulating and omniscient, you will always be calculating, since you know the true treatment effect. [<--**NEEDS WORK - HC**] 

Once we have simulated all the data necessary from our DGP, we can finally move on to calculating our estimand of interest:
[RSHINY/CODE showing ATE]

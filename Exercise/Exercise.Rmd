---
title: "Simulation"
output: learnr::tutorial
runtime: shiny_prerendered

---

```{r setup, include=FALSE}
library(learnr)
library(shiny)
library(tidyverse)
library(plotly)
library(shinyjs)
knitr::opts_chunk$set(echo = FALSE)
```


## Introduction 
**Welcome to the simulation app!**        

This app will walk you through simulating hypothetical samples to test if different causal inference methods are unbiased and efficient in estimating treatment effect.

Throughout the app, we will use a hypothetical real-world example to build your intuition and knowledge about the joys of simulation.

You will have access to two very important hats: the researcher hat and omniscient hat . The researcher hat is one you wear daily - you are in the real world and have normal human limitations. However, every once in a while, you will get to wear the omniscient hat where you will transcend your feeble human mind and become an all-knowing and powerful being. These two hats are very important when we simulate, and will come into play quite often throughout our journey.

Ready? Okay, let's get started!


## Probability Distribution
```{r bernoulli_prob, echo=FALSE}
sliderInput(inputId = "bernoulli_prob",label = "Select the probability of assigning to the treatment group (p):",min = 0, max = 1, value = 0.5, step = 0.1)
actionButton("one_student_treatment", "Assign a student to a group")
textOutput('one_student_treatment_plot')
```

```{r bernoulli_probs, context="server"}
students <- sample(c('James','Robert', 'John','Michael', 'William', 'David', 'Richard', 'Joseph', 'Thomas', 'Charles', 'Christopher', 'Daniel', 'Matthew', 'Anthony', 'Mark', 'Donald', 'Steven', 'Paul', 'Andrew', 'Joshua', 'Kenneth', 'Kevin', 'Brian', 'George', 'Edward', 'Mary', 'Patricia', 'Jennifer', 'Linda', 'Elizabeth', 'Barbara','Yeri', 'Jessica', 'Sarah', 'Karen', 'Nancy', 'Lisa', 'Lee', 'Margaret', 'Sandra', 'Ashley','Kimberly', 'Emily', 'Donna', 'Michelle', 'Dorothy', 'Carol', 'Amanda', 'Melissa', 'Hee-jung','Yichen','Aarav', 'Mohammed', 'Sofía', 'Olivia', 'Lucas', 'Ben', 'Emma', 'Mia', 'Chloé', 'Gabriel', 'Raphaël','Santiago', 'Francisco', 'Leonor', 'Leon', 'Maria', 'Himari', 'Nathaniel', 'Jacob', 'Dalisay', 'Analyn', 'Nur', 'Yuxuan', 'Ahmad', 'Megan', 'Charlotte', 'Xinyi', 'Jack', 'Alex', 'Giulia','Andrea', 'Chiara', 'Marco', 'Hannah', 'Samantha', 'Nathan', 'Simon', 'Camila', 'Juan', 'Afiq','Nurul', 'Haruto', 'Ren', 'Akari', 'Salomé', 'Oliver', 'Aadya', 'Saanvi', 'Yinuo'))

  observeEvent(input$one_student_treatment, {
    treatment <- rbinom(1,size = 1, prob = input$bernoulli_prob)
    student <- sample(students, size = 1)
    output$one_student_treatment_plot <- renderText(paste0(student, ': ', treatment))
  })
```
### Exercise

You need to generate a covariate *frq* that measures how often a student would study for the test on their own in a week. The expected rate of occurrences($lambda$) is 3.       
Please try to generate the covariate using the function rpois(n, lambda), where n is the number of random values to return and lambda is the parameter lambda.

Write the code to generate the covariate. You shall check the hint if you are unfamiliar with R functions:

```{r frq, exercise=TRUE, exercise.eval=TRUE}

```

```{r frq-hint}
frq<-rpois(n= ,lambda =  )
print(frq)
```

```{r frq-solution}
frq<-rpois(n=100,lambda = 3)
print(frq)
```
 
```{r frq-code-check}
grade_code()
```

```{r quiz}
quiz(
  question("Which of following is not an example of continuous variable  :",
    answer("Age of students"),
    answer("Race of students", correct = TRUE),
    answer("Grade of students", correct = TRUE),
    answer("Commute time of students")
  )
)
```

## Sampling Distribution
**What is Sampling Distribution?**       
Suppose you simulated many samples consisting of 100 students drawn from all the students from New York State, and with each sample you calculate a sample mean for 100 pre-treatment scores in order to estimate the population mean or expectation of pre-treatment score in New York State.
A sample mean estimate from one sample is likely to be different from the sample mean estimate from another sample, and these sample means might be higher and lower than the true population mean. 
The sampling distribution of sample mean is the set of possible sample means estimated from all samples of size 100 that could have been observed if the data simulation process had been re-done, along with the probabilities of these possible values.

However, the combinations of 100 students from all students in New York State is an exaodinary large number, and can even exceed the computation capacity of your computer.

For example, say there are 100,000 high school students in New York State and we randomly select 100 students. Here we have population size of 100,000 and sample size of 100. 
How many samples of size 100 are possible out of a population of size 100,000? That's 100,000 choose 100, ${100,000 choose100}$, and the number is so large that even R only returns Inf.

Therefore, we usually use a large number of samples to get an approximate sampling distribution of statistics. For example, below you can simulate a sampling distribution of sample mean by choosing the number of samples, and the population mean and standard deviation of the pre-treatement score.
```{r select_n_sampling_distribution, echo=FALSE}
fluidRow(column(width = 6,sliderInput(inputId = "select_n_sampling_distribution",
                                      label = "Select the number of samples to generate the sampling distribution:",min = 1000, max = 10000, value = 5000, step = 10)),
                      column(width = 6,
                             sliderInput(inputId = "select_mean_normal_sampling",
                                         label = "Select the expectation of the pre-treatment score (E(X)):",
                                         min = 20, max = 80, value = 60, step = 1),
                             sliderInput(inputId = "select_sd_normal_sampling",
                                         label = "Select the standard deviation of the pre-treatment score:",
                                         min = 0, max = 10, value = 5, step = 1)))
             
             actionButton("generate_sampling_distribution", "Generate the Sampling Distribution")
             plotOutput('sampling_distribution_normal')
```

```{r select_n_sampling_distributions, context="server"}
 output$sampling_distribution_normal <- renderPlot({
      input$generate_sampling_distribution
      select_n <- isolate(input$select_n_sampling_distribution)
      select_mean <- isolate(input$select_mean_normal_sampling)
      select_sd <- isolate(input$select_sd_normal_sampling)
      all_means <- data.frame(data = rep(NA, select_n))
      for (i in 1:select_n) {
        sample <- rnorm(n = 100, mean = select_mean, sd = select_sd)
        tmp <- mean(sample)
        all_means$data[i] <- tmp
      }
      ggplot() + geom_histogram(data = all_means, aes(x = data, y = ..density..), bins = 30, alpha = 0.5) +
        geom_vline(xintercept = mean(all_means$data), color = 'blue') 
      
    })
```
### Exercise
You need to generate pre-treatment test scores with two different distribution, normal distribution and uniform distribution. Then, compare two sampling distributions of test scores generated with plots. For normal distribution, we assume the expectation of the pre-treatment score (E(X)) is 50, the standard deviation of the pre-treatment score is 5, and we sample 1000 times.

First, let's generate the sampling distribution of test scores using normal distribution. Modify the following code to generate the data:

```{r clt1, exercise=TRUE, exercise.eval=TRUE}
#create a dataframe to store mean value of each sample

#write a loop to generate sampling distribution
 
```

```{r clt1-hint-1}
#create a dataframe to store mean value of each sample
rnorm_means <- data.frame(data = rep(NA,1000))
#write a loop to generate sampling distribution
  #generate a sample using normal distribution
  #calculate and store the the mean value
```

```{r clt1-hint-2}
#create a dataframe to store mean value of each sample
rnorm_means <- data.frame(data = rep(NA,1000))
#write a loop to generate sampling distribution
for (i in 1: 1000) { 
  #generate a sample using normal distribution
        sample <- rnorm(n = 100, mean = 50, sd = 10)
  #calculate and store the the mean value
        rnorm_means$data[i]
      }
```


```{r clt1-solution}
#create a dataframe to store mean value of each sample
rnorm_means <- data.frame(data = rep(NA, 1000))
#write a loop to generate sampling distribution
for (i in 1:1000) {
  #generate a sample using normal distribution
        sample <- rnorm(n = 100, mean = 50, sd = 5)
  #calculate and store the the mean value
        rnorm_means$data[i]  <- mean(sample)
      }
```
 
Now you shall try to generate the sampling distribution of test scores using uniform distribution. Based on the spread of normal distribution, we can assume the test scores in the uniform distribution runs from 35 to 65. Modify the following code to generate the data:

```{r clt2, exercise=TRUE, exercise.eval=TRUE}
#create a dataframe to store mean value of each sample

#write a loop to generate sampling distribution
 
```


```{r clt2-hint}
#runif generates uniform distribution
#runif(n= number of observations, min = lower limit of the distribution, max = upper limits of the distribution)

```

```{r clt2-solution}
#create a dataframe to store mean value of each sample
uni_means <- data.frame(data = rep(NA, 1000))
#write a loop to generate sampling distribution
for (i in 1:1000) {
  #generate a sample using uniform distribution
        sample <- runif(n = 100, min = 35, max = 65)
  #calculate and store the the mean value
        rnorm_means$data[i]  <- mean(sample)
      }
```

Now, generate the histogram for both sampling distribution. Do you spot any difference?

```{r clt3, exercise=TRUE, exercise.eval=TRUE}
#VZ-need to figure out how to recall previous data/stored data
 
```

```{r clt3-hint}
#Here is the plot for normal distribution. Please overlay the plot of uniform distribution on top of it
# ggplot() + 
# geom_histogram(data = rnorm_means, aes(x = data, y = ..density..), bins = 30, alpha = 0.5) +
#  geom_vline(xintercept = mean(rnorm_means$data), color = 'blue') 
      
```


```{r clt3-solution}
 #ggplot() + 
#geom_histogram(data = rnorm_means, aes(x = data, y = ..density..), bins = 30, alpha = 0.5) +
 # geom_vline(xintercept = mean(rnorm_means$data), color = 'blue') + 
 # geom_histogram(data = uni_means, aes(x = data, y = ..density..), bins = 30, alpha = 0.5,fill = "green") +
# geom_vline(xintercept = mean(uni_means$data), color = 'red') 
      
```



## Simulation

You should use this exercise to test if different treatment effect would effect the bias of different causal inference methods:

Step 1: Data Generating Process (DGP)
```{r simprac, exercise=TRUE, exercise.eval=TRUE}
#set a value for true treatment effect

```

```{r simprac-hint}
#two to three hints
tau <- 5
```

```{r simprac-solution}
tau <- 5 #any values 
#Generate treatment Assignment
Z<-rbinom(n = 100, size = 1, prob = 0.5)
#Generate pre-treatment score
X<-rnorm(n = 100, mean = 50, sd = 5)
#omniscient hat. 
Y0 <- 10 + X + 0 + rnorm(100, mean = 0, sd = 1) 
Y1 <- 10 + X + tau + rnorm(100, mean = 0, sd = 1)
#researcher's heat
Y <- ifelse(Z == 1,  Y1, Y0)
```

Step 2: Calculate SATE
```{r simprac2, exercise=TRUE, exercise.eval=TRUE}
#calculate true SATE

#difference in mean

#Linear Regression to estimate SATE

```

```{r simprac2-hint}
#calculate true SATE
mean(Y1 - Y0)
#difference in mean
mean(Y[Z == ...]) - mean(Y[Z == ...])
#Linear Regression to estimate SATE
fit <- lm(Y ~ X + Z) 
summary(fit)$...


```

```{r simprac2-solution}
#calculate true SATE
mean(Y1 - Y0)
#difference in mean
mean(Y[Z == 1]) - mean(Y[Z == 0])
#Linear Regression to estimate SATE
fit <- lm(Y ~ X + Z) 
summary(fit)$coefficients['Z', 1]

```

Step 3: Comparing Estimators
```{r}
#not sure if we need to build this part into the exercise
```


shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# ------------------------------------------------------------------------------------------------------
# Solution to the example to RAO 7.8
# define a function that fits and returns simple linear model
dgp <- function(a, b, n, sigma){
x <- runif(n = n, min = 0, max = 50)
error <- rnorm(n = n, mean = 0, sd = sigma)
y <- a + b*x + error
fake <- data.frame(y = y, x = x, error = error)
model <- lm(y ~ x, data = fake)
return(model)
}
tmp <- summary(dgp(5, 1, 100, 1))
tmp
# repeat 1000 times
slopes <- c(rep(NA, 1000))
se_slopes <- c(rep(NA, 1000))
for (i in 1:1000){
summary <- summary(dgp(5, 1, 100, 1))
slopes[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
sim_data <- data.frame(slope = slopes, se_slope = se_slopes)
# check that slope is approximately unbiased
mean(sim_data$slope)
# check that standard deviation is approximately equal to standard error
sd(sim_data$slope)
# ------------------------------------------------------------------------------------------------------
# Solution to the example to ALR 2.13
data("cars")
# fit linear regression
fit <- lm(dist ~ speed, data = cars)
summary(fit)
# 99% confidence interval for beta1
confint(fit, level=0.99)
# prediction for a car that has a speed of 20 mph
pi <- predict(fit, data.frame(speed=20), interval="prediction", level=.99)
pi
-17.5791 + 3.9324 *20
# extension: draw 95% confidence intervals for the fitted values, and 95% prediction intervals for the predicted values
ci_95 <- predict(fit, cars, interval="confidence", level=.95)
pi_95 <- predict(fit, cars, interval="prediction", level=.95)
plot(dist~speed, data = cars, pch = 20)
abline(fit)
lines(x = cars$speed, y = ci_95[,2], col = 'red')
lines(x = cars$speed, y = ci_95[,3], col = 'red')
lines(x = cars$speed, y = pi_95[,2], col = 'orange')
lines(x = cars$speed, y = pi_95[,3], col = 'orange')
legend(x = "bottomright",
legend = c("CI", "PI"),
lty = c(1,1),
col = c("red", "orange"),
lwd = 2,
xpd = TRUE)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
dgp <- function(a, b, n, sigma){
x <- runif(n = n, min = 0, max = 50)
error <- rnorm(n = n, mean = 0, sd = sigma)
y <- 5 + x + error
fake <- data.frame(y  = y, x = x, error = error)
model <- lm(y ~ x, data = fake)
return(model)
}
tmp <- summary(dgp(5, 1, 100, 1))
tmp
dgp <- function(a, b, n, sigma){
x <- runif(n = n, min = 0, max = 50)
error <- rnorm(n = n, mean = 0, sd = sigma)
y <- 5 + x + error
fake <- data.frame(y  = y, x = x, error = error)
model <- lm(y ~ x, data = fake)
return(model)
}
tmp <- summary(dgp(5, 1, 100, 1))
# extract estimates and s.e. of estimates
tmp_intercept <- tmp$coefficients[1,1]
tmp_intercept_se <- tmp$coefficients[1,2]
tmp_slope <- tmp$coefficients[2,1]
tmp_slope_se <- tmp$coefficients[2,2]
# repeat 1000 times
slopes <- rep(NA, 1000)
se_slope <- rep(NA, 1000)
for(i in 1:1000){
summaty <- summary(dgp(5, 1, 100, 1))
slope[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
# repeat 1000 times
slopes <- rep(NA, 1000)
se_slope <- rep(NA, 1000)
for(i in 1:1000){
summaty <- summary(dgp(5, 1, 100, 1))
slopes[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
sim_data <- data.frame(slope = slopes, se_slope = se_slopes)
# check the slope estimate is unbiased
mean(sim_data$slope)
# check that sd is approximately equal to s.e.
sd(sim_data$se_slope)
for(i in 1:1000){
summaty <- summary(dgp(5, 1, 100, 1))
slopes[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
sim_data <- data.frame(slope = slopes, se_slope = se_slopes)
# check the slope estimate is unbiased
mean(sim_data$slope)
# check that sd is approximately equal to s.e.
sd(sim_data$se_slope)
tmp
dgp <- function(a, b, n, sigma){
x <- runif(n = n, min = 0, max = 50)
error <- rnorm(n = n, mean = 0, sd = sigma)
y <- 5 + x + error
fake <- data.frame(y  = y, x = x, error = error)
model <- lm(y ~ x, data = fake)
return(model)
}
tmp <- summary(dgp(5, 1, 100, 1))
for(i in 1:1000){
summaty <- summary(dgp(5, 1, 100, 1))
slopes[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
sim_data <- data.frame(slope = slopes, se_slope = se_slopes)
# check the slope estimate is unbiased
mean(sim_data$slope)
# check that sd is approximately equal to s.e.
sd(sim_data$se_slope)
# check that 95% of the estimate Â± 2 standard error interval contains the true parameter value
sim_data$slope_CI_contains_truth <- with(sim_data, ifelse((slope - 2*se_slope) <= 1 & slope + 2*se_slope) >= 1, TRUE, FALSE)
# ------------------------------------------------------------------------------------------------------
# Solution to the example to RAO 7.8
# define a function that fits and returns simple linear model
dgp <- function(a, b, n, sigma){
x <- runif(n = n, min = 0, max = 50)
error <- rnorm(n = n, mean = 0, sd = sigma)
y <- a + b*x + error
fake <- data.frame(y = y, x = x, error = error)
model <- lm(y ~ x, data = fake)
return(model)
}
tmp <- summary(dgp(5, 1, 100, 1))
# repeat 1000 times
slopes <- c(rep(NA, 1000))
se_slopes <- c(rep(NA, 1000))
for (i in 1:1000){
summary <- summary(dgp(5, 1, 100, 1))
slopes[i] <- summary$coefficients[2,1]
se_slopes[i] <- summary$coefficients[2,2]
}
sim_data <- data.frame(slope = slopes, se_slope = se_slopes)
# check that slope is approximately unbiased
mean(sim_data$slope)
# check that standard deviation is approximately equal to standard error
sd(sim_data$slope)
# check that slope is approximately unbiased
mean(sim_data$slope)
# check that standard deviation is approximately equal to standard error
sd(sim_data$slope)
tmp
# check that about 95% of the estimate \pm 2 se intervals contain the true parameter
# use base R
sim_data$slope_CI_contains_truth <- with(sim_data, ifelse((slope - 2*se_slope) <= 1 & (slope + 2*se_slope) >= 1, TRUE, FALSE))
sim_data$slope_CI_contains_truth
# use tidyverse
sim_data <- sim_data %>%
mutate(slope_left_CI_endpoint = slope - 2*se_slope,
slope_right_CI_endpoint = slope + 2*se_slope,
slope_CI_contains_truth = ((slope_left_CI_endpoint <= 1) & (1 <= slope_right_CI_endpoint)))
# load libraries
library(tidyverse)
# use tidyverse
sim_data <- sim_data %>%
mutate(slope_left_CI_endpoint = slope - 2*se_slope,
slope_right_CI_endpoint = slope + 2*se_slope,
slope_CI_contains_truth = ((slope_left_CI_endpoint <= 1) & (1 <= slope_right_CI_endpoint)))
mean(sim_data$slope_CI_contains_truth)
1==1 & 1==2
1==1
1==2
1==1 & 1==2
1:3==1:3
1:3==1:3
1:3==1:3
1:3==c(1,3,3)
1:3==1:3 && 1:3==c(1,3,3)
# Same as above since only 1st elements
1:3==1:3 & 1:3==c(1,3,3)
# Now use AND &
# [TRUE  TRUE  TRUE  TRUE FALSE] & [FALSE FALSE  TRUE  TRUE  TRUE]
x[x<5 & x>2]           # Only 3 and 4 elements are both TRUE
x <- 1:5
x<5
x[x<5]
x<5
x>2
# Now use AND &
# [TRUE  TRUE  TRUE  TRUE FALSE] & [FALSE FALSE  TRUE  TRUE  TRUE]
x[x<5 & x>2]           # Only 3 and 4 elements are both TRUE
x<5
x>2
# Now use AND &
# [TRUE  TRUE  TRUE  TRUE FALSE] & [FALSE FALSE  TRUE  TRUE  TRUE]
x[x<5 | x>2]           # Only 3 and 4 elements are both TRUE
(sim_data$slope - 2*sim_data$se_slope) <= 1
tmp$sigma
tmp
data(cars)
head
head(cars)
# fit
fit <- lm(dist ~ speed, data = cars)
summary(fit)
# 99% CI for coef
confint(fit, level = 0.99)
# predict
-17.5791 + 3.9324  * 20
predict(fit, data.frame(speed = 20), interval = 'prediction')
summary(fit)
# extension: draw 95% confidence intervals for the fitted values, and 95% prediction intervals for the predicted values
ci_95 <- predict(fit, cars, interval="confidence", level=.95)
# extension: draw 95% confidence intervals for the fitted values, and 95% prediction intervals for the predicted values
ci_95 <- predict(fit, cars, interval="confidence", level=.95)
pi_95 <- predict(fit, cars, interval="prediction", level=.95)
plot(dist~speed, data = cars, pch = 20)
abline(fit)
lines(x = cars$speed, y = ci_95[,2], col = 'red')
lines(x = cars$speed, y = ci_95[,3], col = 'red')
lines(x = cars$speed, y = pi_95[,2], col = 'orange')
lines(x = cars$speed, y = pi_95[,3], col = 'orange')
legend(x = "bottomright",
legend = c("CI", "PI"),
lty = c(1,1),
col = c("red", "orange"),
lwd = 2,
xpd = TRUE)
alternative_estimator <- function(y, x){
x_bar <- mean(x)
y_bar <- mean(y)
estimate <- sum((y-y_bar)/(x-x_bar))/length(x)
return(estimate)
}
alternative_estimator <- function(y, x){
x_bar <- mean(x)
y_bar <- mean(y)
estimate <- sum((y-y_bar)/(x-x_bar))/length(x)
return(estimate)
}
a <- 5
b <- 10
sigma <- 2.5
alternative_estimator <- function(y, x){
x_bar <- mean(x)
y_bar <- mean(y)
estimate <- sum((y-y_bar)/(x-x_bar))/length(x)
return(estimate)
}
ols_estimator <-function(y, x){
summary <- summary(lm(y~x))
slope <- summary$coefficients[2,1]
return(slope)
}
n_sim <- 1000
alternative_estimator_list <- rep(NA, n_sim)
ols_estimator_list <- rep(NA, n_sim)
sample_size <- 100
x <- runif(n = sample_size, min = 0, max = 50)
for(i in 1:1000){
y <- a + b*x + rnorm(sample_size, mean = 0, sd = sigma)
alternative_estimator_list[i] <- alternative_estimator(y, x)
ols_estimator_list[i] <- ols_estimator(y, x)
}
mean(alternative_estimator_list)
mean(ols_estimator_list)
sd(alternative_estimator_list)
sd(ols_estimator_list)
shiny::runApp()
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE)
#r packages
library(tidyverse)
library(AER)
?rnorm
